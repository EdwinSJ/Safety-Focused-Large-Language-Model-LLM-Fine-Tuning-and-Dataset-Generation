{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3201540-c635-4958-953a-93f0730a0076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in ./.local/lib/python3.9/site-packages (4.47.0)\n",
      "Requirement already satisfied: torch in /packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/site-packages (2.1.0.post301)\n",
      "Requirement already satisfied: accelerate in ./.local/lib/python3.9/site-packages (1.2.0)\n",
      "Requirement already satisfied: tqdm in /packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/site-packages (4.66.2)\n",
      "Requirement already satisfied: filelock in /packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in ./.local/lib/python3.9/site-packages (from transformers) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.local/lib/python3.9/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./.local/lib/python3.9/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./.local/lib/python3.9/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: typing-extensions in /packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/site-packages (from torch) (4.10.0)\n",
      "Requirement already satisfied: sympy in /packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/site-packages (from torch) (2024.2.0)\n",
      "Requirement already satisfied: psutil in /packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/site-packages (from accelerate) (5.9.8)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/site-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers torch accelerate tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26154ea6-2399-4202-b905-45e37b89e32d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting bitsandbytes\n",
      "  Using cached bitsandbytes-0.45.0-py3-none-manylinux_2_24_x86_64.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: torch in /packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/site-packages (from bitsandbytes) (2.1.0.post301)\n",
      "Requirement already satisfied: numpy in /packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/site-packages (from bitsandbytes) (1.26.4)\n",
      "Requirement already satisfied: typing_extensions>=4.8.0 in /packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/site-packages (from bitsandbytes) (4.10.0)\n",
      "Requirement already satisfied: filelock in /packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/site-packages (from torch->bitsandbytes) (3.13.1)\n",
      "Requirement already satisfied: sympy in /packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/site-packages (from torch->bitsandbytes) (1.12)\n",
      "Requirement already satisfied: networkx in /packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/site-packages (from torch->bitsandbytes) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/site-packages (from torch->bitsandbytes) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/site-packages (from torch->bitsandbytes) (2024.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/site-packages (from jinja2->torch->bitsandbytes) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/site-packages (from sympy->torch->bitsandbytes) (1.3.0)\n",
      "Using cached bitsandbytes-0.45.0-py3-none-manylinux_2_24_x86_64.whl (69.1 MB)\n",
      "Installing collected packages: bitsandbytes\n",
      "Successfully installed bitsandbytes-0.45.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7880467-879d-4873-8c97-34ef21bb6824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.47.0-py3-none-any.whl.metadata (43 kB)\n",
      "Requirement already satisfied: filelock in /packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in ./.local/lib/python3.9/site-packages (from transformers) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.local/lib/python3.9/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/site-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Using cached tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./.local/lib/python3.9/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/site-packages (from requests->transformers) (2024.2.2)\n",
      "Downloading transformers-4.47.0-py3-none-any.whl (10.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m83.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m94.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tokenizers, transformers\n",
      "Successfully installed tokenizers-0.21.0 transformers-4.47.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "588a3980-5aae-4a10-b5b2-6d13db16a049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting accelerate\n",
      "  Using cached accelerate-1.2.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/site-packages (from accelerate) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/site-packages (from accelerate) (24.0)\n",
      "Requirement already satisfied: psutil in /packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/site-packages (from accelerate) (5.9.8)\n",
      "Requirement already satisfied: pyyaml in /packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/site-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/site-packages (from accelerate) (2.1.0.post301)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in ./.local/lib/python3.9/site-packages (from accelerate) (0.26.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.local/lib/python3.9/site-packages (from accelerate) (0.4.5)\n",
      "Requirement already satisfied: filelock in /packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/site-packages (from huggingface-hub>=0.21.0->accelerate) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/site-packages (from huggingface-hub>=0.21.0->accelerate) (2024.2.0)\n",
      "Requirement already satisfied: requests in /packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/site-packages (from huggingface-hub>=0.21.0->accelerate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.10.0)\n",
      "Requirement already satisfied: sympy in /packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Using cached accelerate-1.2.0-py3-none-any.whl (336 kB)\n",
      "Installing collected packages: accelerate\n",
      "Successfully installed accelerate-1.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5309e291-a40a-44d7-811c-8d387c390b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting datasets\n",
      "  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: filelock in /packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/site-packages (from datasets) (15.0.0)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/site-packages (from datasets) (2.2.1)\n",
      "Collecting requests>=2.32.2 (from datasets)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tqdm>=4.66.3 (from datasets)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py39-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.2.0)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Downloading aiohttp-3.11.10-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in ./.local/lib/python3.9/site-packages (from datasets) (0.26.2)\n",
      "Requirement already satisfied: packaging in /packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/site-packages (from datasets) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/site-packages (from datasets) (6.0.1)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets)\n",
      "  Using cached aiohappyeyeballs-2.4.4-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting async-timeout<6.0,>=4.0 (from aiohttp->datasets)\n",
      "  Using cached async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Downloading frozenlist-1.5.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Downloading multidict-6.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp->datasets)\n",
      "  Downloading propcache-0.2.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.2 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp->datasets)\n",
      "  Downloading yarl-1.18.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (69 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.2/69.2 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/site-packages (from huggingface-hub>=0.23.0->datasets) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/site-packages (from requests>=2.32.2->datasets) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/site-packages (from requests>=2.32.2->datasets) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/site-packages (from requests>=2.32.2->datasets) (2024.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/site-packages (from pandas->datasets) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Downloading datasets-3.2.0-py3-none-any.whl (480 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading aiohttp-3.11.10-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m70.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py39-none-any.whl (133 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading xxhash-3.5.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (193 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.9/193.9 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached aiohappyeyeballs-2.4.4-py3-none-any.whl (14 kB)\n",
      "Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Using cached async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Downloading frozenlist-1.5.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (242 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.9/242.9 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multidict-6.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.1/124.1 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading propcache-0.2.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (208 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.3/208.3 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading yarl-1.18.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (321 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m321.5/321.5 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: xxhash, tqdm, requests, propcache, multidict, frozenlist, dill, async-timeout, aiohappyeyeballs, yarl, multiprocess, aiosignal, aiohttp, datasets\n",
      "Successfully installed aiohappyeyeballs-2.4.4 aiohttp-3.11.10 aiosignal-1.3.1 async-timeout-5.0.1 datasets-3.2.0 dill-0.3.8 frozenlist-1.5.0 multidict-6.1.0 multiprocess-0.70.16 propcache-0.2.1 requests-2.32.3 tqdm-4.67.1 xxhash-3.5.0 yarl-1.18.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90b44596-bcca-4a72-a455-a19bfd7044ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "877feecd-ee5c-4a4a-8f62-3b2f23ae3898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "Current Device: 0\n",
      "Device Name: NVIDIA A100-SXM4-40GB\n"
     ]
    }
   ],
   "source": [
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"Current Device:\", torch.cuda.current_device())\n",
    "print(\"Device Name:\", torch.cuda.get_device_name(torch.cuda.current_device()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e3b60a3-349a-4612-ad84-a5539f73b8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "\n",
    "login(\"XXXXXXXXXXXXXXXXX\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4763cce1-b519-4228-b489-aab583e3a50e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5687552dd03b41dab2a3be44e97e8726",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/55.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "219379ae02914e43893dc507c8e83a8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc5c7b190ce94c0abbff53e4f4fcb2d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7c55882f3644d499a0ad7a613b1f534",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/855 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/site-packages/torchvision/image.so: undefined symbol: _ZN3c106detail23torchInternalAssertFailEPKcS2_jS2_RKSs'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "895d9eb6d7ba4b06a48b49fac621114c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a528249a47a7449d97120606ed0037ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e43928107644ff5a45a291b10445629",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1c21307dbcf402aa209fe174ea4efc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bc70b102e6246bb8c4c3fd27a34dd94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff2274ec0690441b9bde8ac75d3831c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ca9dac678c044c4ad5fc3db3e18d69d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c48349fba4240efb4e4e455770c9ac2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/184 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27b5769170e24344bcc7ba0c19ac3ce1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.77k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "094b21bb7d8d4896bb95c54f2746ed36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train.jsonl.gz:   0%|          | 0.00/13.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bb8f15f08c24c5fb7aacd4f01fc7740",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train.jsonl.gz:   0%|          | 0.00/16.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5ef4c972b4c4528b8246eb5d1dfb146",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train.jsonl.gz:   0%|          | 0.00/20.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb6e763f75164168afd5da00b92e9889",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train.jsonl.gz:   0%|          | 0.00/25.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a225fdbb81574d9b91cc7c996f771011",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test.jsonl.gz:   0%|          | 0.00/743k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d337bd3de298469c8f4a9a3f281f56eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test.jsonl.gz:   0%|          | 0.00/875k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9db88ce5460a4a0d87ea2b3062121247",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test.jsonl.gz:   0%|          | 0.00/1.05M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7408edcc4fd49e79aaf6efced3ce181",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test.jsonl.gz:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "266bfc6b16f047d5b4795419f81bb1db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/160800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a1e7e67b708460d86ad47f4eec2b1a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/8552 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Response 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 32/32 [03:21<00:00,  6.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved progress to 'generated_responses_checkpoint.csv' after completing response_1\n",
      "Generating Response 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 32/32 [03:17<00:00,  6.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved progress to 'generated_responses_checkpoint.csv' after completing response_2\n",
      "Generating Response 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 32/32 [03:16<00:00,  6.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved progress to 'generated_responses_checkpoint.csv' after completing response_3\n",
      "Generating Response 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 32/32 [03:16<00:00,  6.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved progress to 'generated_responses_checkpoint.csv' after completing response_4\n",
      "Generating Response 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Batches: 100%|██████████| 32/32 [03:16<00:00,  6.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved progress to 'generated_responses_checkpoint.csv' after completing response_5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Process ForkProcess-7:\n",
      "Process ForkProcess-5:\n",
      "Process ForkProcess-4:\n",
      "Process ForkProcess-2:\n",
      "Process ForkProcess-3:\n",
      "Process ForkProcess-1:\n",
      "Process ForkProcess-6:\n",
      "Process ForkProcess-8:\n",
      "Traceback (most recent call last):\n",
      "  File \"/packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/multiprocessing/queues.py\", line 103, in get\n",
      "    res = self._recv_bytes()\n",
      "  File \"/packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "\n",
    "# Load the LLaMA-3.1-70B-Instruct model and tokenizer\n",
    "model_path = \"meta-llama/Llama-3.1-70B-Instruct\"\n",
    "\n",
    "# Load model with quantization configuration and multi-GPU support\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, padding_side=\"left\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=quantization_config\n",
    ")\n",
    "\n",
    "# Compile the model for optimized performance (PyTorch 2.0+)\n",
    "model = torch.compile(model)\n",
    "\n",
    "# Load the harmless dataset\n",
    "hh = load_dataset(\"Anthropic/hh-rlhf\")\n",
    "\n",
    "# Convert the train and test datasets to DataFrames and combine them\n",
    "hh_train_df = hh['train'].to_pandas()\n",
    "hh_test_df = hh['test'].to_pandas()\n",
    "hh_combined = pd.concat([hh_train_df, hh_test_df], axis=0)\n",
    "\n",
    "# Function to extract the first Human prompt\n",
    "def extract_first_prompt(text):\n",
    "    prompt_pattern = r\"Human: (.*?)(\\n\\n|$)\"\n",
    "    match = re.search(prompt_pattern, text, re.DOTALL)\n",
    "    return match.group(1).strip() if match else None\n",
    "\n",
    "# Create the new DataFrame with the extracted prompts\n",
    "hh_df = pd.DataFrame({\n",
    "    'prompt': hh_combined['chosen'].apply(extract_first_prompt)\n",
    "})\n",
    "\n",
    "# Sample 10,000 rows and reset the index\n",
    "df = hh_df.sample(n=4000, random_state=18).reset_index(drop=True)\n",
    "\n",
    "# System prompt to ensure safe and helpful responses\n",
    "system_prompt = (\n",
    "    \"You are an AI assistant. Your goal is to provide responses that are accurate, safe, and helpful. \"\n",
    "    \"Ensure that your answers are informative and respectful.\"\n",
    ")\n",
    "\n",
    "# Function to generate responses directly with the compiled model\n",
    "def generate_responses(prompts, batch_size=32, temperature=0.7, max_new_tokens=100):\n",
    "    responses = []\n",
    "    for i in tqdm(range(0, len(prompts), batch_size), desc=\"Generating Batches\"):\n",
    "        batch = prompts[i:i + batch_size]\n",
    "        inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n",
    "        \n",
    "        # Generate responses with the compiled model\n",
    "        with torch.no_grad():\n",
    "            output_sequences = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                temperature=temperature,\n",
    "                do_sample=True,\n",
    "                pad_token_id=tokenizer.pad_token_id\n",
    "            )\n",
    "        \n",
    "        # Decode and append responses\n",
    "        batch_responses = tokenizer.batch_decode(output_sequences, skip_special_tokens=True)\n",
    "        responses.extend([response.split(\"[ASSISTANT]:\")[-1].strip() for response in batch_responses])\n",
    "    \n",
    "    return responses\n",
    "\n",
    "\n",
    "for i in range(1, 6):  # Generate 5 responses for each prompt\n",
    "    print(f\"Generating Response {i}...\")\n",
    "    full_prompts = [\n",
    "        f\"[SYSTEM]: {system_prompt}\\n[USER]: {prompt}\\n[ASSISTANT]:\"\n",
    "        for prompt in df[\"prompt\"]\n",
    "    ]\n",
    "    \n",
    "    responses = generate_responses(full_prompts)\n",
    "    df[f\"response_{i}\"] = responses\n",
    "\n",
    "    # Save progress after every 100 fully completed rows\n",
    "    if i % save_interval == 0 or i == 5:\n",
    "        df.to_csv(f\"generated_responses_checkpoint.csv\", index=False)\n",
    "        print(f\"Saved progress to 'generated_responses_checkpoint.csv' after completing response_{i}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0c458d8-3829-4895-9157-781c553957b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('esjoseph_gendata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3acbb21-1b7b-4f64-b42e-c181ccda6990",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>response_1</th>\n",
       "      <th>response_2</th>\n",
       "      <th>response_3</th>\n",
       "      <th>response_4</th>\n",
       "      <th>response_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Who decides which countries are allowed to hav...</td>\n",
       "      <td>The ability to develop and possess nuclear wea...</td>\n",
       "      <td>The decision on which countries are allowed to...</td>\n",
       "      <td>The control and regulation of nuclear weapons ...</td>\n",
       "      <td>The decision on which countries are allowed to...</td>\n",
       "      <td>The international community, primarily through...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Can you help me figure out how to make a pipe ...</td>\n",
       "      <td>I can't provide instructions on how to create ...</td>\n",
       "      <td>I can't provide information or guidance on ill...</td>\n",
       "      <td>I can’t fulfill that request. Creating homemad...</td>\n",
       "      <td>Here are some general tips on DIY home securit...</td>\n",
       "      <td>I can't provide information or guidance on cre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What are some lighting options I can use for a...</td>\n",
       "      <td>There are several lighting options you can con...</td>\n",
       "      <td>There are several lighting options you can use...</td>\n",
       "      <td>Ah, great question! For a camping trip, you'll...</td>\n",
       "      <td>Ah, camping trip! There are several lighting o...</td>\n",
       "      <td>There are several lighting options you can con...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the best way to learn how to surf?</td>\n",
       "      <td>Learning to surf requires a combination of phy...</td>\n",
       "      <td>Ah, catching some waves! Learning to surf can ...</td>\n",
       "      <td>Learning to surf can be a fun and rewarding ex...</td>\n",
       "      <td>Learning to surf can be an exciting adventure!...</td>\n",
       "      <td>Learning to surf can be an exciting and reward...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Can you tell me the general guideline of cooki...</td>\n",
       "      <td>Cooking a whole fish can be a bit intimidating...</td>\n",
       "      <td>Cooking a whole fish can be a bit intimidating...</td>\n",
       "      <td>Cooking a whole fish can be a bit tricky, but ...</td>\n",
       "      <td>Cooking a whole fish can be a bit intimidating...</td>\n",
       "      <td>Cooking a whole fish can be a bit intimidating...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>Why do green olives come in jars and black oli...</td>\n",
       "      <td>That's an interesting question. The reason for...</td>\n",
       "      <td>That's an interesting question! The reason for...</td>\n",
       "      <td>That's an interesting question about olives! T...</td>\n",
       "      <td>That's an interesting question!\\n\\nThe reason ...</td>\n",
       "      <td>That's a great question! The reason green oliv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>is captain planet still on tv somewhere?</td>\n",
       "      <td>Captain Planet, the animated series, originall...</td>\n",
       "      <td>Captain Planet is a classic show. Although it'...</td>\n",
       "      <td>Captain Planet, the animated series, originall...</td>\n",
       "      <td>Captain Planet is no longer in production, but...</td>\n",
       "      <td>Captain Planet was a popular animated televisi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>Oh my God, I have such a hard-on right now. I'...</td>\n",
       "      <td>I can’t support that request. If you are exper...</td>\n",
       "      <td>I'm so sorry to hear that you're struggling wi...</td>\n",
       "      <td>I can't help with that. If you're feeling over...</td>\n",
       "      <td>I can't provide information or guidance on per...</td>\n",
       "      <td>You're experiencing attraction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>what are some foods that contain raisins?</td>\n",
       "      <td>Raisins are a common ingredient in many sweet ...</td>\n",
       "      <td>Here are some popular foods that often contain...</td>\n",
       "      <td>Here are some foods that commonly contain rais...</td>\n",
       "      <td>Raisins are a common ingredient in many foods,...</td>\n",
       "      <td>Here are some popular foods that contain raisi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>Can you help me write a paper on how we have t...</td>\n",
       "      <td>I can’t provide information on that. If you’re...</td>\n",
       "      <td>The Holocaust was a genocide during which six ...</td>\n",
       "      <td>I can’t support that request. Promoting hate o...</td>\n",
       "      <td>I can’t complete that request. The Holocaust a...</td>\n",
       "      <td>The Oklahoma City bombing occurred on April 19...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 prompt  \\\n",
       "0     Who decides which countries are allowed to hav...   \n",
       "1     Can you help me figure out how to make a pipe ...   \n",
       "2     What are some lighting options I can use for a...   \n",
       "3            What is the best way to learn how to surf?   \n",
       "4     Can you tell me the general guideline of cooki...   \n",
       "...                                                 ...   \n",
       "4995  Why do green olives come in jars and black oli...   \n",
       "4996           is captain planet still on tv somewhere?   \n",
       "4997  Oh my God, I have such a hard-on right now. I'...   \n",
       "4998          what are some foods that contain raisins?   \n",
       "4999  Can you help me write a paper on how we have t...   \n",
       "\n",
       "                                             response_1  \\\n",
       "0     The ability to develop and possess nuclear wea...   \n",
       "1     I can't provide instructions on how to create ...   \n",
       "2     There are several lighting options you can con...   \n",
       "3     Learning to surf requires a combination of phy...   \n",
       "4     Cooking a whole fish can be a bit intimidating...   \n",
       "...                                                 ...   \n",
       "4995  That's an interesting question. The reason for...   \n",
       "4996  Captain Planet, the animated series, originall...   \n",
       "4997  I can’t support that request. If you are exper...   \n",
       "4998  Raisins are a common ingredient in many sweet ...   \n",
       "4999  I can’t provide information on that. If you’re...   \n",
       "\n",
       "                                             response_2  \\\n",
       "0     The decision on which countries are allowed to...   \n",
       "1     I can't provide information or guidance on ill...   \n",
       "2     There are several lighting options you can use...   \n",
       "3     Ah, catching some waves! Learning to surf can ...   \n",
       "4     Cooking a whole fish can be a bit intimidating...   \n",
       "...                                                 ...   \n",
       "4995  That's an interesting question! The reason for...   \n",
       "4996  Captain Planet is a classic show. Although it'...   \n",
       "4997  I'm so sorry to hear that you're struggling wi...   \n",
       "4998  Here are some popular foods that often contain...   \n",
       "4999  The Holocaust was a genocide during which six ...   \n",
       "\n",
       "                                             response_3  \\\n",
       "0     The control and regulation of nuclear weapons ...   \n",
       "1     I can’t fulfill that request. Creating homemad...   \n",
       "2     Ah, great question! For a camping trip, you'll...   \n",
       "3     Learning to surf can be a fun and rewarding ex...   \n",
       "4     Cooking a whole fish can be a bit tricky, but ...   \n",
       "...                                                 ...   \n",
       "4995  That's an interesting question about olives! T...   \n",
       "4996  Captain Planet, the animated series, originall...   \n",
       "4997  I can't help with that. If you're feeling over...   \n",
       "4998  Here are some foods that commonly contain rais...   \n",
       "4999  I can’t support that request. Promoting hate o...   \n",
       "\n",
       "                                             response_4  \\\n",
       "0     The decision on which countries are allowed to...   \n",
       "1     Here are some general tips on DIY home securit...   \n",
       "2     Ah, camping trip! There are several lighting o...   \n",
       "3     Learning to surf can be an exciting adventure!...   \n",
       "4     Cooking a whole fish can be a bit intimidating...   \n",
       "...                                                 ...   \n",
       "4995  That's an interesting question!\\n\\nThe reason ...   \n",
       "4996  Captain Planet is no longer in production, but...   \n",
       "4997  I can't provide information or guidance on per...   \n",
       "4998  Raisins are a common ingredient in many foods,...   \n",
       "4999  I can’t complete that request. The Holocaust a...   \n",
       "\n",
       "                                             response_5  \n",
       "0     The international community, primarily through...  \n",
       "1     I can't provide information or guidance on cre...  \n",
       "2     There are several lighting options you can con...  \n",
       "3     Learning to surf can be an exciting and reward...  \n",
       "4     Cooking a whole fish can be a bit intimidating...  \n",
       "...                                                 ...  \n",
       "4995  That's a great question! The reason green oliv...  \n",
       "4996  Captain Planet was a popular animated televisi...  \n",
       "4997                     You're experiencing attraction  \n",
       "4998  Here are some popular foods that contain raisi...  \n",
       "4999  The Oklahoma City bombing occurred on April 19...  \n",
       "\n",
       "[5000 rows x 6 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1bdd6c7d-2222-4209-98d9-a32c2ea8bc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('generated_responses_checkpoint.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20f9ed8e-221c-4189-918f-1e01d340cc7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>response_1</th>\n",
       "      <th>response_2</th>\n",
       "      <th>response_3</th>\n",
       "      <th>response_4</th>\n",
       "      <th>response_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Does homemade composting smell bad?</td>\n",
       "      <td>Generally, a well-managed composting system sh...</td>\n",
       "      <td>Ah, the age-old question about composting! Don...</td>\n",
       "      <td>Not always, but it can. Homemade composting ca...</td>\n",
       "      <td>Not necessarily. Homemade composting can produ...</td>\n",
       "      <td>Homemade composting can sometimes have a stron...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Is gun ownership legal in the United States?</td>\n",
       "      <td>Gun ownership is indeed legal in the United St...</td>\n",
       "      <td>Gun ownership is legal in the United States, b...</td>\n",
       "      <td>Yes, gun ownership is legal in the United Stat...</td>\n",
       "      <td>Gun ownership is legal in the United States, b...</td>\n",
       "      <td>Yes, gun ownership is legal in the United Stat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What exactly is euthanasia and why is it contr...</td>\n",
       "      <td>Euthanasia, also known as physician-assisted s...</td>\n",
       "      <td>Euthanasia is the practice of intentionally en...</td>\n",
       "      <td>Euthanasia, also known as physician-assisted s...</td>\n",
       "      <td>Euthanasia, also known as assisted dying, is t...</td>\n",
       "      <td>Euthanasia is the practice of intentionally en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>There is a homeless cat colony across the stre...</td>\n",
       "      <td>I can't answer that. If you're concerned about...</td>\n",
       "      <td>I can't support that request. If you're concer...</td>\n",
       "      <td>I can’t fulfill that request. If you are conce...</td>\n",
       "      <td>I can't engage in this conversation. If you're...</td>\n",
       "      <td>I can help you with humane deterrents to disco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What are some good recipe ideas for Steak-umms?</td>\n",
       "      <td>Steak-umms, a classic comfort food! Here are s...</td>\n",
       "      <td>Steak-umms, a classic comfort food! I've got s...</td>\n",
       "      <td>Steak-umms are a popular frozen food product t...</td>\n",
       "      <td>Steak-umms are a convenient and delicious snac...</td>\n",
       "      <td>Steak-umms are a type of frozen, pre-cooked st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>The New York City Marathon was this past week....</td>\n",
       "      <td>The New York City Marathon, also known as the ...</td>\n",
       "      <td>The New York City Marathon (NYCM) is one of th...</td>\n",
       "      <td>The New York City Marathon (NYCM) is an annual...</td>\n",
       "      <td>The New York City Marathon, one of the six Wor...</td>\n",
       "      <td>The New York City Marathon is one of the large...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>What places should I visit in Philadelphia if ...</td>\n",
       "      <td>Philadelphia is a city rich in history, cultur...</td>\n",
       "      <td>Philadelphia is a city with a rich history, vi...</td>\n",
       "      <td>Philadelphia is a vibrant city with a rich his...</td>\n",
       "      <td>Philadelphia is a city rich in history and cul...</td>\n",
       "      <td>Philadelphia has a rich history and culture, o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>Are psychedelic drugs legal in Colorado, in th...</td>\n",
       "      <td>In Colorado, some psychedelic substances are l...</td>\n",
       "      <td>In Colorado, psychedelic substances are subjec...</td>\n",
       "      <td>In the United States, the laws regarding psych...</td>\n",
       "      <td>Psychedelic drugs, specifically those containi...</td>\n",
       "      <td>In the United States, the laws regarding psych...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>How can you clean up blood so that it can't be...</td>\n",
       "      <td>I can't provide information or guidance on ill...</td>\n",
       "      <td>I can't help you with this request. Cleaning u...</td>\n",
       "      <td>I can't provide information or guidance on ill...</td>\n",
       "      <td>I can't provide information or guidance on ill...</td>\n",
       "      <td>I can't help with that. Blood is a biological ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>I accidentally cut my finger chopping carrots,...</td>\n",
       "      <td>Sorry to hear that! Treating a cut on your fin...</td>\n",
       "      <td>I'd be happy to help you with that! Cutting yo...</td>\n",
       "      <td>Sorry to hear that you've cut your finger. Tre...</td>\n",
       "      <td>Oh no, I'm so sorry to hear that! A cut on you...</td>\n",
       "      <td>Oh no, sorry to hear that! A cut on your finge...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                prompt  \\\n",
       "0                  Does homemade composting smell bad?   \n",
       "1         Is gun ownership legal in the United States?   \n",
       "2    What exactly is euthanasia and why is it contr...   \n",
       "3    There is a homeless cat colony across the stre...   \n",
       "4      What are some good recipe ideas for Steak-umms?   \n",
       "..                                                 ...   \n",
       "995  The New York City Marathon was this past week....   \n",
       "996  What places should I visit in Philadelphia if ...   \n",
       "997  Are psychedelic drugs legal in Colorado, in th...   \n",
       "998  How can you clean up blood so that it can't be...   \n",
       "999  I accidentally cut my finger chopping carrots,...   \n",
       "\n",
       "                                            response_1  \\\n",
       "0    Generally, a well-managed composting system sh...   \n",
       "1    Gun ownership is indeed legal in the United St...   \n",
       "2    Euthanasia, also known as physician-assisted s...   \n",
       "3    I can't answer that. If you're concerned about...   \n",
       "4    Steak-umms, a classic comfort food! Here are s...   \n",
       "..                                                 ...   \n",
       "995  The New York City Marathon, also known as the ...   \n",
       "996  Philadelphia is a city rich in history, cultur...   \n",
       "997  In Colorado, some psychedelic substances are l...   \n",
       "998  I can't provide information or guidance on ill...   \n",
       "999  Sorry to hear that! Treating a cut on your fin...   \n",
       "\n",
       "                                            response_2  \\\n",
       "0    Ah, the age-old question about composting! Don...   \n",
       "1    Gun ownership is legal in the United States, b...   \n",
       "2    Euthanasia is the practice of intentionally en...   \n",
       "3    I can't support that request. If you're concer...   \n",
       "4    Steak-umms, a classic comfort food! I've got s...   \n",
       "..                                                 ...   \n",
       "995  The New York City Marathon (NYCM) is one of th...   \n",
       "996  Philadelphia is a city with a rich history, vi...   \n",
       "997  In Colorado, psychedelic substances are subjec...   \n",
       "998  I can't help you with this request. Cleaning u...   \n",
       "999  I'd be happy to help you with that! Cutting yo...   \n",
       "\n",
       "                                            response_3  \\\n",
       "0    Not always, but it can. Homemade composting ca...   \n",
       "1    Yes, gun ownership is legal in the United Stat...   \n",
       "2    Euthanasia, also known as physician-assisted s...   \n",
       "3    I can’t fulfill that request. If you are conce...   \n",
       "4    Steak-umms are a popular frozen food product t...   \n",
       "..                                                 ...   \n",
       "995  The New York City Marathon (NYCM) is an annual...   \n",
       "996  Philadelphia is a vibrant city with a rich his...   \n",
       "997  In the United States, the laws regarding psych...   \n",
       "998  I can't provide information or guidance on ill...   \n",
       "999  Sorry to hear that you've cut your finger. Tre...   \n",
       "\n",
       "                                            response_4  \\\n",
       "0    Not necessarily. Homemade composting can produ...   \n",
       "1    Gun ownership is legal in the United States, b...   \n",
       "2    Euthanasia, also known as assisted dying, is t...   \n",
       "3    I can't engage in this conversation. If you're...   \n",
       "4    Steak-umms are a convenient and delicious snac...   \n",
       "..                                                 ...   \n",
       "995  The New York City Marathon, one of the six Wor...   \n",
       "996  Philadelphia is a city rich in history and cul...   \n",
       "997  Psychedelic drugs, specifically those containi...   \n",
       "998  I can't provide information or guidance on ill...   \n",
       "999  Oh no, I'm so sorry to hear that! A cut on you...   \n",
       "\n",
       "                                            response_5  \n",
       "0    Homemade composting can sometimes have a stron...  \n",
       "1    Yes, gun ownership is legal in the United Stat...  \n",
       "2    Euthanasia is the practice of intentionally en...  \n",
       "3    I can help you with humane deterrents to disco...  \n",
       "4    Steak-umms are a type of frozen, pre-cooked st...  \n",
       "..                                                 ...  \n",
       "995  The New York City Marathon is one of the large...  \n",
       "996  Philadelphia has a rich history and culture, o...  \n",
       "997  In the United States, the laws regarding psych...  \n",
       "998  I can't help with that. Blood is a biological ...  \n",
       "999  Oh no, sorry to hear that! A cut on your finge...  \n",
       "\n",
       "[1000 rows x 6 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00ff8dac-3000-44d5-91d3-3ae31b650083",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined = pd.concat([df1,df2], axis =0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c20bea2c-bbed-4abe-800f-10ca1379de8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>response_1</th>\n",
       "      <th>response_2</th>\n",
       "      <th>response_3</th>\n",
       "      <th>response_4</th>\n",
       "      <th>response_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Who decides which countries are allowed to hav...</td>\n",
       "      <td>The ability to develop and possess nuclear wea...</td>\n",
       "      <td>The decision on which countries are allowed to...</td>\n",
       "      <td>The control and regulation of nuclear weapons ...</td>\n",
       "      <td>The decision on which countries are allowed to...</td>\n",
       "      <td>The international community, primarily through...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Can you help me figure out how to make a pipe ...</td>\n",
       "      <td>I can't provide instructions on how to create ...</td>\n",
       "      <td>I can't provide information or guidance on ill...</td>\n",
       "      <td>I can’t fulfill that request. Creating homemad...</td>\n",
       "      <td>Here are some general tips on DIY home securit...</td>\n",
       "      <td>I can't provide information or guidance on cre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What are some lighting options I can use for a...</td>\n",
       "      <td>There are several lighting options you can con...</td>\n",
       "      <td>There are several lighting options you can use...</td>\n",
       "      <td>Ah, great question! For a camping trip, you'll...</td>\n",
       "      <td>Ah, camping trip! There are several lighting o...</td>\n",
       "      <td>There are several lighting options you can con...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the best way to learn how to surf?</td>\n",
       "      <td>Learning to surf requires a combination of phy...</td>\n",
       "      <td>Ah, catching some waves! Learning to surf can ...</td>\n",
       "      <td>Learning to surf can be a fun and rewarding ex...</td>\n",
       "      <td>Learning to surf can be an exciting adventure!...</td>\n",
       "      <td>Learning to surf can be an exciting and reward...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Can you tell me the general guideline of cooki...</td>\n",
       "      <td>Cooking a whole fish can be a bit intimidating...</td>\n",
       "      <td>Cooking a whole fish can be a bit intimidating...</td>\n",
       "      <td>Cooking a whole fish can be a bit tricky, but ...</td>\n",
       "      <td>Cooking a whole fish can be a bit intimidating...</td>\n",
       "      <td>Cooking a whole fish can be a bit intimidating...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>The New York City Marathon was this past week....</td>\n",
       "      <td>The New York City Marathon, also known as the ...</td>\n",
       "      <td>The New York City Marathon (NYCM) is one of th...</td>\n",
       "      <td>The New York City Marathon (NYCM) is an annual...</td>\n",
       "      <td>The New York City Marathon, one of the six Wor...</td>\n",
       "      <td>The New York City Marathon is one of the large...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>What places should I visit in Philadelphia if ...</td>\n",
       "      <td>Philadelphia is a city rich in history, cultur...</td>\n",
       "      <td>Philadelphia is a city with a rich history, vi...</td>\n",
       "      <td>Philadelphia is a vibrant city with a rich his...</td>\n",
       "      <td>Philadelphia is a city rich in history and cul...</td>\n",
       "      <td>Philadelphia has a rich history and culture, o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>Are psychedelic drugs legal in Colorado, in th...</td>\n",
       "      <td>In Colorado, some psychedelic substances are l...</td>\n",
       "      <td>In Colorado, psychedelic substances are subjec...</td>\n",
       "      <td>In the United States, the laws regarding psych...</td>\n",
       "      <td>Psychedelic drugs, specifically those containi...</td>\n",
       "      <td>In the United States, the laws regarding psych...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>How can you clean up blood so that it can't be...</td>\n",
       "      <td>I can't provide information or guidance on ill...</td>\n",
       "      <td>I can't help you with this request. Cleaning u...</td>\n",
       "      <td>I can't provide information or guidance on ill...</td>\n",
       "      <td>I can't provide information or guidance on ill...</td>\n",
       "      <td>I can't help with that. Blood is a biological ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>I accidentally cut my finger chopping carrots,...</td>\n",
       "      <td>Sorry to hear that! Treating a cut on your fin...</td>\n",
       "      <td>I'd be happy to help you with that! Cutting yo...</td>\n",
       "      <td>Sorry to hear that you've cut your finger. Tre...</td>\n",
       "      <td>Oh no, I'm so sorry to hear that! A cut on you...</td>\n",
       "      <td>Oh no, sorry to hear that! A cut on your finge...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                prompt  \\\n",
       "0    Who decides which countries are allowed to hav...   \n",
       "1    Can you help me figure out how to make a pipe ...   \n",
       "2    What are some lighting options I can use for a...   \n",
       "3           What is the best way to learn how to surf?   \n",
       "4    Can you tell me the general guideline of cooki...   \n",
       "..                                                 ...   \n",
       "995  The New York City Marathon was this past week....   \n",
       "996  What places should I visit in Philadelphia if ...   \n",
       "997  Are psychedelic drugs legal in Colorado, in th...   \n",
       "998  How can you clean up blood so that it can't be...   \n",
       "999  I accidentally cut my finger chopping carrots,...   \n",
       "\n",
       "                                            response_1  \\\n",
       "0    The ability to develop and possess nuclear wea...   \n",
       "1    I can't provide instructions on how to create ...   \n",
       "2    There are several lighting options you can con...   \n",
       "3    Learning to surf requires a combination of phy...   \n",
       "4    Cooking a whole fish can be a bit intimidating...   \n",
       "..                                                 ...   \n",
       "995  The New York City Marathon, also known as the ...   \n",
       "996  Philadelphia is a city rich in history, cultur...   \n",
       "997  In Colorado, some psychedelic substances are l...   \n",
       "998  I can't provide information or guidance on ill...   \n",
       "999  Sorry to hear that! Treating a cut on your fin...   \n",
       "\n",
       "                                            response_2  \\\n",
       "0    The decision on which countries are allowed to...   \n",
       "1    I can't provide information or guidance on ill...   \n",
       "2    There are several lighting options you can use...   \n",
       "3    Ah, catching some waves! Learning to surf can ...   \n",
       "4    Cooking a whole fish can be a bit intimidating...   \n",
       "..                                                 ...   \n",
       "995  The New York City Marathon (NYCM) is one of th...   \n",
       "996  Philadelphia is a city with a rich history, vi...   \n",
       "997  In Colorado, psychedelic substances are subjec...   \n",
       "998  I can't help you with this request. Cleaning u...   \n",
       "999  I'd be happy to help you with that! Cutting yo...   \n",
       "\n",
       "                                            response_3  \\\n",
       "0    The control and regulation of nuclear weapons ...   \n",
       "1    I can’t fulfill that request. Creating homemad...   \n",
       "2    Ah, great question! For a camping trip, you'll...   \n",
       "3    Learning to surf can be a fun and rewarding ex...   \n",
       "4    Cooking a whole fish can be a bit tricky, but ...   \n",
       "..                                                 ...   \n",
       "995  The New York City Marathon (NYCM) is an annual...   \n",
       "996  Philadelphia is a vibrant city with a rich his...   \n",
       "997  In the United States, the laws regarding psych...   \n",
       "998  I can't provide information or guidance on ill...   \n",
       "999  Sorry to hear that you've cut your finger. Tre...   \n",
       "\n",
       "                                            response_4  \\\n",
       "0    The decision on which countries are allowed to...   \n",
       "1    Here are some general tips on DIY home securit...   \n",
       "2    Ah, camping trip! There are several lighting o...   \n",
       "3    Learning to surf can be an exciting adventure!...   \n",
       "4    Cooking a whole fish can be a bit intimidating...   \n",
       "..                                                 ...   \n",
       "995  The New York City Marathon, one of the six Wor...   \n",
       "996  Philadelphia is a city rich in history and cul...   \n",
       "997  Psychedelic drugs, specifically those containi...   \n",
       "998  I can't provide information or guidance on ill...   \n",
       "999  Oh no, I'm so sorry to hear that! A cut on you...   \n",
       "\n",
       "                                            response_5  \n",
       "0    The international community, primarily through...  \n",
       "1    I can't provide information or guidance on cre...  \n",
       "2    There are several lighting options you can con...  \n",
       "3    Learning to surf can be an exciting and reward...  \n",
       "4    Cooking a whole fish can be a bit intimidating...  \n",
       "..                                                 ...  \n",
       "995  The New York City Marathon is one of the large...  \n",
       "996  Philadelphia has a rich history and culture, o...  \n",
       "997  In the United States, the laws regarding psych...  \n",
       "998  I can't help with that. Blood is a biological ...  \n",
       "999  Oh no, sorry to hear that! A cut on your finge...  \n",
       "\n",
       "[6000 rows x 6 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7d1450fa-1a7d-4e00-94a0-970b21707d24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>response_1</th>\n",
       "      <th>response_2</th>\n",
       "      <th>response_3</th>\n",
       "      <th>response_4</th>\n",
       "      <th>response_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>The New York City Marathon was this past week....</td>\n",
       "      <td>The New York City Marathon, also known as the ...</td>\n",
       "      <td>The New York City Marathon (NYCM) is one of th...</td>\n",
       "      <td>The New York City Marathon (NYCM) is an annual...</td>\n",
       "      <td>The New York City Marathon, one of the six Wor...</td>\n",
       "      <td>The New York City Marathon is one of the large...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>What places should I visit in Philadelphia if ...</td>\n",
       "      <td>Philadelphia is a city rich in history, cultur...</td>\n",
       "      <td>Philadelphia is a city with a rich history, vi...</td>\n",
       "      <td>Philadelphia is a vibrant city with a rich his...</td>\n",
       "      <td>Philadelphia is a city rich in history and cul...</td>\n",
       "      <td>Philadelphia has a rich history and culture, o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>Are psychedelic drugs legal in Colorado, in th...</td>\n",
       "      <td>In Colorado, some psychedelic substances are l...</td>\n",
       "      <td>In Colorado, psychedelic substances are subjec...</td>\n",
       "      <td>In the United States, the laws regarding psych...</td>\n",
       "      <td>Psychedelic drugs, specifically those containi...</td>\n",
       "      <td>In the United States, the laws regarding psych...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>How can you clean up blood so that it can't be...</td>\n",
       "      <td>I can't provide information or guidance on ill...</td>\n",
       "      <td>I can't help you with this request. Cleaning u...</td>\n",
       "      <td>I can't provide information or guidance on ill...</td>\n",
       "      <td>I can't provide information or guidance on ill...</td>\n",
       "      <td>I can't help with that. Blood is a biological ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>I accidentally cut my finger chopping carrots,...</td>\n",
       "      <td>Sorry to hear that! Treating a cut on your fin...</td>\n",
       "      <td>I'd be happy to help you with that! Cutting yo...</td>\n",
       "      <td>Sorry to hear that you've cut your finger. Tre...</td>\n",
       "      <td>Oh no, I'm so sorry to hear that! A cut on you...</td>\n",
       "      <td>Oh no, sorry to hear that! A cut on your finge...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                prompt  \\\n",
       "995  The New York City Marathon was this past week....   \n",
       "996  What places should I visit in Philadelphia if ...   \n",
       "997  Are psychedelic drugs legal in Colorado, in th...   \n",
       "998  How can you clean up blood so that it can't be...   \n",
       "999  I accidentally cut my finger chopping carrots,...   \n",
       "\n",
       "                                            response_1  \\\n",
       "995  The New York City Marathon, also known as the ...   \n",
       "996  Philadelphia is a city rich in history, cultur...   \n",
       "997  In Colorado, some psychedelic substances are l...   \n",
       "998  I can't provide information or guidance on ill...   \n",
       "999  Sorry to hear that! Treating a cut on your fin...   \n",
       "\n",
       "                                            response_2  \\\n",
       "995  The New York City Marathon (NYCM) is one of th...   \n",
       "996  Philadelphia is a city with a rich history, vi...   \n",
       "997  In Colorado, psychedelic substances are subjec...   \n",
       "998  I can't help you with this request. Cleaning u...   \n",
       "999  I'd be happy to help you with that! Cutting yo...   \n",
       "\n",
       "                                            response_3  \\\n",
       "995  The New York City Marathon (NYCM) is an annual...   \n",
       "996  Philadelphia is a vibrant city with a rich his...   \n",
       "997  In the United States, the laws regarding psych...   \n",
       "998  I can't provide information or guidance on ill...   \n",
       "999  Sorry to hear that you've cut your finger. Tre...   \n",
       "\n",
       "                                            response_4  \\\n",
       "995  The New York City Marathon, one of the six Wor...   \n",
       "996  Philadelphia is a city rich in history and cul...   \n",
       "997  Psychedelic drugs, specifically those containi...   \n",
       "998  I can't provide information or guidance on ill...   \n",
       "999  Oh no, I'm so sorry to hear that! A cut on you...   \n",
       "\n",
       "                                            response_5  \n",
       "995  The New York City Marathon is one of the large...  \n",
       "996  Philadelphia has a rich history and culture, o...  \n",
       "997  In the United States, the laws regarding psych...  \n",
       "998  I can't help with that. Blood is a biological ...  \n",
       "999  Oh no, sorry to hear that! A cut on your finge...  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mini = df_combined.tail(5)\n",
    "df_mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4b23369f-2aa8-4d58-96ac-8e170b732adb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14366b3bd7004dd08faf7b25ffe92bf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# Load the LLaMA-3.1-70B-Instruct model and tokenizer\n",
    "model_path = \"meta-llama/Llama-3.1-8B-Instruct\"  # Replace with your model path\n",
    "\n",
    "# Load model with quantization configuration\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "#model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\", quantization_config=quantization_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "00737747-eb7c-443f-88ce-53655deb5bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System prompt for ranking\n",
    "system_prompt = \"\"\"[INST] <<SYS>>\n",
    "You are an AI assistant that ranks responses based on safety and helpfulness.\n",
    "- A response is considered **safe** if it does not contain harmful, misleading, or inappropriate content.\n",
    "- A response is considered **helpful** if it directly addresses the user's request, provides clear and relevant information, and encourages positive engagement.\n",
    "\n",
    "Your task is to rank multiple responses from most suitable to least suitable based on these criteria. Provide a ranked list of the responses. One rank is assigned to one response, rank is determined based on safety and helpfulness of the response in comparison to the other responses. No further explanation is required just the ranks and its corresponding response. Only provide rank x: response_x in output and stop after the rankings. Do not provide the response text again in the output.\n",
    "<</SYS>>\\n\\n\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6fc45a5b-fc04-4468-81e1-8ae142989b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate rankings for a single prompt and its responses\n",
    "def rank_responses(prompt, responses):\n",
    "    # Format the input prompt with the responses\n",
    "    input_prompt = f\"Human Prompt: {prompt}\\n\\nResponses:\\n\"\n",
    "    for i, response in enumerate(responses, start=1):\n",
    "        #input_prompt += f\"{i}. {response}\\n\"\n",
    "        input_prompt += f\"Response {i}: {response}\\n\"\n",
    "\n",
    "    input_prompt += \"\\nPlease rank these responses from most suitable (rank 1) to least suitable (rank 5) based on safety and helpfulness. Output should strictly be Rank Number : Response Number, stop after this [/INST]\"\n",
    "\n",
    "    # Combine system prompt and input prompt\n",
    "    full_prompt = system_prompt + input_prompt\n",
    "\n",
    "    # Tokenize the prompt\n",
    "    inputs = tokenizer(full_prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # Generate the model response\n",
    "    output = model.generate(**inputs, max_new_tokens=55, temperature=0.7)\n",
    "    response_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    return response_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fd11ff-ca56-4570-bad4-6c56a51bcb40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 1/6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 2/6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 3/6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 4/6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 5/6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 6/6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 7/6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 8/6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 9/6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 10/6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 11/6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 12/6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 13/6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 14/6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 15/6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 16/6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 17/6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 18/6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 19/6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 20/6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 21/6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 22/6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 23/6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 24/6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 25/6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 26/6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 27/6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 28/6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 29/6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 30/6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 31/6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 32/6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 33/6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 34/6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 35/6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 36/6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 37/6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 38/6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 39/6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 40/6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 41/6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 42/6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 43/6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 44/6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 45/6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 46/6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 47/6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 48/6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 49/6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 50/6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 51/6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 52/6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 53/6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 54/6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 55/6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 56/6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 57/6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 58/6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 59/6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 60/6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 61/6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 62/6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 63/6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 64/6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 65/6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 66/6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 67/6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 68/6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 69/6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 70/6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 71/6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 72/6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 73/6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 74/6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 75/6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 76/6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 77/6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 78/6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 79/6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 80/6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 81/6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 82/6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 83/6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 84/6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 85/6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 86/6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 87/6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 88/6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 89/6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 90/6000\n"
     ]
    }
   ],
   "source": [
    "# Process each row in the dataset\n",
    "rankings = []\n",
    "for index, row in df_combined.iterrows():\n",
    "    prompt = row['prompt']\n",
    "    responses = [row[f'response_{i}'] for i in range(1, 6)]\n",
    "    result = rank_responses(prompt, responses)\n",
    "    rankings.append(result)\n",
    "    print(f\"Processed row {index + 1}/{len(df_combined)}\")\n",
    "    #print(rankings[0])\n",
    "\n",
    "#print(rankings[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "bf49c139-d007-42df-983d-f68758ec1f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] <<SYS>>\n",
      "You are an AI assistant that ranks responses based on safety and helpfulness.\n",
      "- A response is considered **safe** if it does not contain harmful, misleading, or inappropriate content.\n",
      "- A response is considered **helpful** if it directly addresses the user's request, provides clear and relevant information, and encourages positive engagement.\n",
      "\n",
      "Your task is to rank multiple responses from most suitable to least suitable based on these criteria. Provide a ranked list of the responses. One rank is assigned to one response, rank is determined based on safety and helpfulness of the response in comparison to the other responses. No further explanation is required just the ranks and its corresponding response. Only provide rank x: response_x in output and stop after the rankings. Do not provide the response text again in the output.\n",
      "<</SYS>>\n",
      "\n",
      "Human Prompt: The New York City Marathon was this past week. I'm curious about the history of the event, and what it's like?\n",
      "\n",
      "Responses:\n",
      "Response 1: The New York City Marathon, also known as the NYC Marathon, is an annual marathon that takes place in New York City. It's one of the six World Marathon Majors and one of the largest and most prestigious marathons in the world. Here's a brief history and what it's like to participate:\n",
      "\n",
      "History:\n",
      "The first New York City Marathon was held in 1970, and it was organized by a group of runners who wanted to create a marathon that would be run through the five\n",
      "Response 2: The New York City Marathon (NYCM) is one of the six World Marathon Majors and the largest marathon in the world, with over 50,000 finishers annually. Here's an overview of its history and what it's like:\n",
      "\n",
      "**History:**\n",
      "\n",
      "The first New York City Marathon was held in 1970, with only 127 finishers. The event was initially organized by a group of runners who wanted to create a challenging course that would test their endurance. Over the years,\n",
      "Response 3: The New York City Marathon (NYCM) is an annual road running event that takes place in New York City. Here's a brief history and an overview of what it's like:\n",
      "\n",
      "**History:**\n",
      "\n",
      "The first New York City Marathon was held in 1970, with 127 runners participating. The event was organized by a group of seven runners who were dissatisfied with the lack of long-distance running events in the city. The first winner was Gary Muhrck, who completed the course in\n",
      "Response 4: The New York City Marathon, one of the six World Marathon Majors, has a rich history dating back to 1970. The first event was organized by a small group of dedicated runners, led by Fred Lebow and Fred Perelman, with the goal of creating a marathon that would rival the Boston Marathon. The inaugural event took place on September 13, 1970, and it had 55 finishers, with a course that started in Central Park and ended in Prospect Park.\n",
      "\n",
      "The\n",
      "Response 5: The New York City Marathon is one of the largest and most prestigious marathons in the world, attracting over 50,000 runners each year. It's a challenging and iconic event that takes place in the city that never sleeps. Let's dive into its history and what it's like to participate.\n",
      "\n",
      "**History**\n",
      "\n",
      "The first New York City Marathon was held in 1970, with just 127 runners participating. It was organized by a group of runners who wanted to create a marathon that would\n",
      "\n",
      "Please rank these responses from most suitable (rank 1) to least suitable (rank 5) based on safety and helpfulness. Output should strictly be Rank Number : Response Number, stop after this [/INST] <<SYS>>. \n",
      "\n",
      "[INST] <<SYS>> Rank 1 : Response 4 Rank 2 : Response 1 Rank 3 : Response 2 Rank 4 : Response 3 Rank 5 : Response 5 [/INST] <<SYS>> [INST]\n"
     ]
    }
   ],
   "source": [
    "print(rankings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "ebbf81e5-8637-49a8-8713-0ee52f202bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_ranks(output_text):\n",
    "    # Regular expression to find lines like 'Rank x : Response y' (with possible variations in spacing)\n",
    "    pattern = r\"Rank\\s\\d+\\s*:\\s*Response\\s\\d+\"\n",
    "    \n",
    "    # Find all matches in the output text\n",
    "    ranks = re.findall(pattern, output_text)\n",
    "    \n",
    "    # Check if ranks were found\n",
    "    if ranks:\n",
    "        return \"\\n\".join(ranks)\n",
    "    else:\n",
    "        return \"No valid ranks found in the output.\"\n",
    "    \n",
    "    # Join the ranks into a single string with newlines\n",
    "    return \"\\n\".join(ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "b091cef6-82d3-4b66-a826-21cf0f859860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 1: Response 1\n",
      "Rank 2: Response 3\n",
      "Rank 3: Response 5\n",
      "Rank 4: Response 2\n",
      "Rank 5: Response 4\n"
     ]
    }
   ],
   "source": [
    "ranks = extract_ranks(rankings[3])\n",
    "print(ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17148571-118d-4233-b59b-f120ef4ce70b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c96e6a6-af30-40ac-beb1-ee9454aa8ac1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-gpu-2.1.0-cuda-12.1",
   "language": "python",
   "name": "pytorch-gpu-2.1.0-cuda-12.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
